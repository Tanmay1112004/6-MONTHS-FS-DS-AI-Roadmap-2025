{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV-ZuHL22o0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "eGDdMuV7Q1hG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yG81Y2roFm-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THIS BELOW CODE WILL RUN IN JUPYTER NOTEBOOK\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, RegexParser\n",
        "\n",
        " # Download necessery NLTK data files (only need to this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# sample text\n",
        "text = \"Full stack datascience, generative ai, agentic ai, llm model keep increse by different company\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Define a chunk grammr\n",
        "chunk_grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN>}  # Noun phrase\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>*}  # Verb phrase\n",
        "    PP: {<IN><NP>}  # Prepositional phrase\n",
        "\"\"\"\n",
        "\n",
        "# Create a chunk parser\n",
        "chunk_parser = RegexParser(chunk_grammar)\n",
        "\n",
        "# Parse the tagged tokens\n",
        "chunked = chunk_parser.parse(tagged_tokens)\n",
        "\n",
        "# Print the chunked output\n",
        "print(chunked)\n",
        "\n",
        "# optinally , you cam visulize the chunks\n",
        "chunked.draw()\n",
        "\n"
      ],
      "metadata": {
        "id": "n3ym7p-2Whso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK (only needed if not pre-installed)\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # For newer NLTK versions\n",
        "\n",
        "# Sample text\n",
        "text = \"Full stack datascience, generative ai, agentic ai, llm model keep increase by different company\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Display POS-tagged tokens\n",
        "print(\"POS Tagged Tokens:\")\n",
        "print(tagged_tokens)\n",
        "\n",
        "# Define chunk grammar\n",
        "chunk_grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN.*>}        # Noun Phrase\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>*}   # Verb Phrase\n",
        "    PP: {<IN><NP>}                # Prepositional Phrase\n",
        "\"\"\"\n",
        "\n",
        "# Create a chunk parser\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# Parse the tagged tokens\n",
        "chunked = chunk_parser.parse(tagged_tokens)\n",
        "\n",
        "# Display parsed chunks\n",
        "print(\"\\nChunked Structure:\")\n",
        "print(chunked)\n",
        "\n",
        "# NOTE: chunked.draw() won't work in Colab since it requires a GUI.\n",
        "# To visualize chunks in Colab, you'd need to extract them programmatically instead.\n"
      ],
      "metadata": {
        "id": "UEKc3d65FnBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bc4de2-b4dc-48bd-823b-b6e7ee642c43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagged Tokens:\n",
            "[('Full', 'NNP'), ('stack', 'NN'), ('datascience', 'NN'), (',', ','), ('generative', 'JJ'), ('ai', 'NN'), (',', ','), ('agentic', 'JJ'), ('ai', 'NN'), (',', ','), ('llm', 'JJ'), ('model', 'NN'), ('keep', 'VB'), ('increase', 'NN'), ('by', 'IN'), ('different', 'JJ'), ('company', 'NN')]\n",
            "\n",
            "Chunked Structure:\n",
            "(S\n",
            "  (NP Full/NNP)\n",
            "  (NP stack/NN)\n",
            "  (NP datascience/NN)\n",
            "  ,/,\n",
            "  (NP generative/JJ ai/NN)\n",
            "  ,/,\n",
            "  (NP agentic/JJ ai/NN)\n",
            "  ,/,\n",
            "  (NP llm/JJ model/NN)\n",
            "  (VP keep/VB (NP increase/NN))\n",
            "  (PP by/IN (NP different/JJ company/NN)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQC1Yj3ZFnEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM in chunking"
      ],
      "metadata": {
        "id": "j3yZLu3bZEmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Q6hm8JfBFnLJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "eee64b8f-0136-49a0-922b-10e04b6fc7b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can replace with any other LLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid a warning\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "\n",
        "def chunk_text(text, max_length=512):\n",
        "    \"\"\"Chunk text into smaller pieces.\"\"\"\n",
        "    tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), max_length):\n",
        "        chunk = tokens[i:i + max_length]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def generate_responses(chunks):\n",
        "    \"\"\"Generate responses for each chunk using the LLM.\"\"\"\n",
        "    responses = []\n",
        "    for chunk in chunks:\n",
        "        input_ids = chunk.unsqueeze(0)  # Add batch dimension\n",
        "        # Use max_new_tokens instead of max_length to control the length of the generated response\n",
        "        output = model.generate(input_ids, max_new_tokens=100)  # Generate response\n",
        "        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "\n",
        "    return responses\n",
        "\n",
        "# Example long text\n",
        "long_text = \"Your long text goes here. \" * 50  # Repeat to simulate long text\n",
        "\n",
        "# Chunk the text\n",
        "chunks = chunk_text(long_text)\n",
        "\n",
        "# Generate responses for each chunk\n",
        "responses = generate_responses(chunks)\n",
        "\n",
        "# Print the responses\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Response for chunk {i+1}:\\n{response}\\n\")"
      ],
      "metadata": {
        "id": "9q-p0nc9FnQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da663c9-5035-46a5-b068-6895fac947c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for chunk 1:\n",
            "Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. Your long text goes here. _________________________________________________________________________\n",
            "\n",
            "RAW Paste Data\n",
            "\n",
            "# This file was automatically generated by Mod Organizer. Skyrim.esm Update.esm Dawnguard.esm HearthFires.esm Dragonborn.esm Unofficial Skyrim Legendary Edition Patch.esp Unofficial Dawnguard Patch.esp Unofficial HearthFires Patch.esp Unofficial Dragonborn Patch.esp Unofficial HearthFires Patch.esp Unofficial HighResTexturePack01.esp Unofficial HighResTexturePack02.esp Unofficial HighRes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yLcRS2dFnSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEc5dGy3FnYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpBb41y5Fncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETudx4xRFngA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4xR5X7QYU32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8QrSVulYfHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWaX3-iVYZxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHiXVyA3YZuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qL1DpV-DYZpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZS-Dhgh8YZkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vYg2cCPYZY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}