# ğŸ¬ Next Word Generation using LSTM

A **Deep Learningâ€“powered Next Word Prediction system** trained on the **TMDB 5000 Movies Dataset**.
This project leverages **LSTM (Long Short-Term Memory)** networks to predict and generate the next words in a sequence with contextual understanding.

To make it practical (and fun), the model is wrapped inside an **interactive Gradio web interface**, allowing real-time experimentation with text generation parameters.

---

## âœ¨ Highlights

* End-to-end **text preprocessing & tokenization**
* **LSTM-based neural network** for next-word prediction
* Training visualization with **loss & accuracy curves**
* Model persistence using `.h5` format
* **Interactive Gradio UI**

  * Adjustable number of predicted words
  * Top-K word selection for controlled randomness
* Sleek **dark-themed demo interface**

---

## ğŸ§  How It Works (High Level)

1. Movie titles are tokenized and converted into sequences
2. LSTM learns word dependencies and contextual patterns
3. Given a seed phrase, the model predicts the most probable next words
4. Gradio provides a real-time playground to test predictions

Simple pipeline. Solid ML fundamentals. Clean execution.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Next_Word_Prediction.ipynb    # Colab notebook (training + inference)
â”œâ”€â”€ nwp.h5                       # Trained LSTM model
â”œâ”€â”€ tmdb_5000_movies.csv         # Dataset (from Kaggle)
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸ› ï¸ Tech Stack

* **Python** ğŸ
* **TensorFlow / Keras**
* **Pandas & NumPy**
* **Gradio** (Interactive frontend)
* **Matplotlib** (Training visualizations)

---

## âš¡ Quick Start

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Tanmay1112004/Next-Word-Generation-LSTM.git
cd Next-Word-Generation-LSTM
```

### 2ï¸âƒ£ Open in Google Colab

Upload:

* Notebook
* Dataset
* Model file (`nwp.h5`)

### 3ï¸âƒ£ Install dependencies

```bash
!pip install tensorflow gradio matplotlib pandas numpy
```

### 4ï¸âƒ£ Run all cells

Train (or load) the model and initialize the UI.

### 5ï¸âƒ£ Launch the Gradio app

```python
demo.launch(share=True)
```

Boom. Live demo ready ğŸš€

---

## ğŸ¯ Sample Prediction

**Seed Text**

```
The Lord
```

**Model Output (Next 5 Words)**

```
The Lord of the Rings
```

Clean prediction. Context locked in ğŸ”

---

## ğŸ“Š Training Insights

* Accuracy and loss curves are plotted directly in the notebook
* Helps monitor convergence and detect overfitting early

This keeps the model honest and production-ready.

---

## ğŸ“Œ Use Cases

* Text autocomplete systems
* NLP learning projects
* Language modeling experiments
* AI-powered content tools

---

## ğŸ¤ Acknowledgements

* **Dataset:** TMDB 5000 Movies Dataset (Kaggle)
  [https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata)

---

## â­ Support

If this project helped you:

* **Star the repository**
* Share feedback
* Connect with me on **LinkedIn**
  ğŸ‘‰ [https://www.linkedin.com/tanmay-kshirsagar](https://www.linkedin.com/tanmay-kshirsagar)

Letâ€™s build smarter NLP systems â€” one word at a time ğŸ’¡ğŸ”¥

---

